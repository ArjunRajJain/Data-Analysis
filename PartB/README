For this part I used python to analyze the data.
I first import the data from sql using sqlalchemy and pandas.
I then normalized the data by removing unnecessary columns (such as empty ones)
and by scaling any numbers (with respect to their column).
I also added an extra column indicating good/bad (1/0) based upon their
status being Current/FullyPaid or not.
I then tried to tackle the problem of overfitting by
using cross_validation to choose our test/train data for us, so that I don't bias
the test/train sets. At first I had manually split the data into good/bad and then
split it again into perfect half and half test/training sets. This was resulting
in pretty high accuracy levels, hence why I felt like I was overfitting.
I wanted to spend more time to vectorize the strings and include them in the data
as well, as I feel like state of residence and term could also be influencing factors.

I analyzed the data with the following algorithms -

    Gaussian Naive Bayes :
        I used this next because I wanted to test if it was probable that there
        is independence between each feature and after getting an average accuracy
        of 0.90 (+/- 0.00) I wasn't fully convinced.

    Logistic Regression :
        I then used Logistic Regression for simplicity sake to see if there was
        an easy weighting to each (scaled) feature, which there was as indicated
        by an average accuracy of 0.90 (+/- 0.00).

    K Nearest Neighbor :
        I used this because I wanted to test if neighbors (in terms of euclidean
        distance in the data) were a good indication of a loan's class.We got an
        average accuracy of about 0.85 (+/- 0.01), indicating that euclidean
        distance between numerical fields is definitely a defining factor in
        classifying a loan.

    Random Forest Classifier :
        I finally thought I'd try a Random Forest Classifier as I wanted to use
        a decision tree to find which features are truly the most important
        and by using a Random Forest Classifier I got the benefit of reducing my
        likelihood of overfitting. With an accuracy of 0.97 (+/- 0.01), I gotta
        say I was pretty happy with the results.


You can install the appropriate pip packages by running the following -

pip install -r requirements.txt

and then you can run the code by executing python numerical.py

the folders weights/predictions hold the appropriate data from the algorithms.

The input folder was used in the past to load the data into python, but now
thats done straight from sql.
